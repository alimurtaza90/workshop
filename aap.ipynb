{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = \"./data/docs\"\n",
    "documents = load_documents(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"documents is a {type(documents)}, of length {len(documents)}, where each element is a {type(documents[0])} object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for paragraphs and newlines\n",
    "split_pattern = r\"\\r?\\n\\s*\\r?\\n+\"\n",
    "\n",
    "# Initialize lists to store the word counts of all chunks (now paragraphs) and entire texts across all documents\n",
    "chunk_word_counts = []\n",
    "entire_text_word_counts = []\n",
    "\n",
    "# Initialize a variable to count the total number of paragraphs\n",
    "total_paragraph_count = 0\n",
    "\n",
    "# Iterate through each Document object in your list of documents\n",
    "for doc in documents:\n",
    "    # Assuming doc.text contains the full text of the PDF document\n",
    "    paragraphs = re.split(split_pattern, doc.text)\n",
    "    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip()]\n",
    "\n",
    "    # Update the total paragraph count\n",
    "    total_paragraph_count += len(paragraphs)\n",
    "\n",
    "    # Calculate the number of words in each paragraph and store it\n",
    "    chunk_word_counts.extend([len(paragraph.split()) for paragraph in paragraphs])\n",
    "\n",
    "    # Calculate the number of words in the entire text and store it\n",
    "    entire_word_count = len(doc.text.split())\n",
    "    entire_text_word_counts.append(entire_word_count)\n",
    "\n",
    "# Calculate summary statistics for paragraphs\n",
    "average_paragraph_word_count = sum(chunk_word_counts) / len(chunk_word_counts)\n",
    "max_paragraph_word_count = max(chunk_word_counts)\n",
    "\n",
    "# Calculate average word count for entire texts\n",
    "average_entire_text_word_count = sum(entire_text_word_counts) / len(entire_text_word_counts)\n",
    "\n",
    "# Calculate the average number of paragraphs per document\n",
    "average_paragraphs_per_document = total_paragraph_count / len(documents)\n",
    "\n",
    "print(f\"Average word count for a document: {average_entire_text_word_count}\")\n",
    "print(f\"Average word count per paragraph: {average_paragraph_word_count}\")\n",
    "print(f\"Longest paragraph: {max_paragraph_word_count}\")\n",
    "print(f\"Total number of paragraphs: {total_paragraph_count}\")\n",
    "print(f\"Average number of paragraphs per document: {average_paragraphs_per_document}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    # paragraph_separator=r\"\\r?\\n\\s*\\r?\\n+\", \n",
    "    chunk_size=512, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the character count and word count for each node and getting avergaes\n",
    "node_char_counts = [len(node.text) for node in nodes]\n",
    "node_word_counts = [len(node.text.split()) for node in nodes]\n",
    "\n",
    "average_node_char_count = sum(node_char_counts) / len(node_char_counts)\n",
    "average_node_word_count = sum(node_word_counts) / len(node_word_counts)\n",
    "\n",
    "print(f\"Average character count for a node: {average_node_char_count}\")\n",
    "print(f\"Average word count for a node: {average_node_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all metadata values with keys for one node\n",
    "for key in nodes[0].metadata.keys():\n",
    "    print(f\"{key}: {nodes[0].metadata[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Configuration for llm, embedding model, and node parsing\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "Settings.llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=300.0)\n",
    "Settings.embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embedding_model))\n",
    "\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    if documents:\n",
    "        print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "def build_index(client, documents, index_name):\n",
    "\n",
    "    chroma_collection = client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    print(f\"Created/existing collection {chroma_collection}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents,\n",
    "        transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# save documents to vector store\n",
    "def run_ingestion_pipeline(vectoredb_path, docs_path):\n",
    "    \n",
    "    print(\"Connecting to ChromaDB...\")\n",
    "    chromadb_client = chromadb.PersistentClient(path=vectoredb_path)\n",
    "\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents(docs_path)\n",
    "\n",
    "    print(\"Building index...\")\n",
    "    index = build_index(\n",
    "        client=chromadb_client, \n",
    "        documents=documents, \n",
    "        index_name=\"test\"\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data\"\n",
    "\n",
    "run_ingestion_pipeline(vectordb_path, docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# def retrieve_from_index(chunk_size, llm, embed_model, chromadb_client, index_name):\n",
    "def retrieve_from_index(chromadb_client, index_name):\n",
    "    chroma_collection = chromadb_client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        # chunk_size=chunk_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.llms.ollama  import Ollama\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "# from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data\"\n",
    "index_name = \"test\"\n",
    "chunk_size = 500\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "print(\"Connecting to Chromadb\")\n",
    "chromadb_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "\n",
    "print(\"Loading Ollama...\")\n",
    "llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=300.0)\n",
    "\n",
    "print(\"Retrieving index...\")\n",
    "index = retrieve_from_index(chromadb_client, index_name)\n",
    "\n",
    "print(\"Constructing query engine...\")\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index=index,\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is Buddhism?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
