{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Chunks?\n",
    "Chunks are the building blocks of RAG. They are the smallest unit of information that can be stored in Vector database. This information could be a single word, a sentence, a paragraph, or even an entire document. Chunks are stored in a way that allows them to be easily retrieved and manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = \"./data/docs\"\n",
    "documents = load_documents(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"documents is a {type(documents)}, of length {len(documents)}, where each element is a {type(documents[0])} object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Startegy\n",
    "\n",
    "### Regex pattern used: \n",
    "#### Pattern: `r'\\r?\\n\\s*\\r?\\n+'`\n",
    "#### Explanation:\n",
    "Let's break down this pattern to understand each component:\n",
    "\n",
    "- \\r?: This part of the pattern matches zero or one occurrence of a carriage return (\\r). The question mark ? makes the carriage return optional, which allows the pattern to work with both Windows-style line endings (\\r\\n, where the line ends with both a carriage return and a newline character) and Unix-style line endings (\\n, where the line ends with just a newline character).\n",
    "\n",
    "- \\n: This matches a newline character. Combined with the preceding \\r?, this part matches a line break that could be represented either as \\r\\n (Windows) or \\n (Unix/Linux).\n",
    "\n",
    "- \\s*: This matches any whitespace characters (including spaces, tabs, and line breaks) zero or more times. The asterisk * denotes \"zero or more occurrences,\" allowing for any number of whitespace characters, including none, between line breaks. This is useful for catching cases where paragraphs might be separated by one or more blank lines, possibly containing spaces or tabs.\n",
    "\n",
    "- \\r?\\n: This is similar to the first part, matching another optional carriage return followed by a newline character, indicating the end of the blank line(s) and the start of a new paragraph.\n",
    "\n",
    "- +: Placed at the end of the pattern, this quantifier matches one or more occurrences of the preceding pattern. It ensures that the regex can match multiple consecutive paragraph breaks, treating them as a single split point. This is useful for separating paragraphs that might be divided by more than one blank line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for paragraphs and newlines\n",
    "split_pattern = r\"\\r?\\n\\s*\\r?\\n+\"\n",
    "\n",
    "# Initialize lists to store the word counts of all chunks (now paragraphs) and entire texts across all documents\n",
    "chunk_word_counts = []\n",
    "entire_text_word_counts = []\n",
    "\n",
    "# Initialize a variable to count the total number of paragraphs\n",
    "total_paragraph_count = 0\n",
    "\n",
    "# Iterate through each Document object in your list of documents\n",
    "for doc in documents:\n",
    "    # Assuming doc.text contains the full text of the PDF document\n",
    "    paragraphs = re.split(split_pattern, doc.text)\n",
    "    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip()]\n",
    "\n",
    "    # Update the total paragraph count\n",
    "    total_paragraph_count += len(paragraphs)\n",
    "\n",
    "    # Calculate the number of words in each paragraph and store it\n",
    "    chunk_word_counts.extend([len(paragraph.split()) for paragraph in paragraphs])\n",
    "\n",
    "    # Calculate the number of words in the entire text and store it\n",
    "    entire_word_count = len(doc.text.split())\n",
    "    entire_text_word_counts.append(entire_word_count)\n",
    "\n",
    "# Calculate summary statistics for paragraphs\n",
    "average_paragraph_word_count = sum(chunk_word_counts) / len(chunk_word_counts)\n",
    "max_paragraph_word_count = max(chunk_word_counts)\n",
    "\n",
    "# Calculate average word count for entire texts\n",
    "average_entire_text_word_count = sum(entire_text_word_counts) / len(entire_text_word_counts)\n",
    "\n",
    "# Calculate the average number of paragraphs per document\n",
    "average_paragraphs_per_document = total_paragraph_count / len(documents)\n",
    "\n",
    "print(f\"Average word count for a document: {average_entire_text_word_count}\")\n",
    "print(f\"Average word count per paragraph: {average_paragraph_word_count}\")\n",
    "print(f\"Longest paragraph: {max_paragraph_word_count}\")\n",
    "print(f\"Total number of paragraphs: {total_paragraph_count}\")\n",
    "print(f\"Average number of paragraphs per document: {average_paragraphs_per_document}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    # paragraph_separator=r\"\\r?\\n\\s*\\r?\\n+\", \n",
    "    chunk_size=512, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the character count and word count for each node and getting avergaes\n",
    "node_char_counts = [len(node.text) for node in nodes]\n",
    "node_word_counts = [len(node.text.split()) for node in nodes]\n",
    "\n",
    "average_node_char_count = sum(node_char_counts) / len(node_char_counts)\n",
    "average_node_word_count = sum(node_word_counts) / len(node_word_counts)\n",
    "\n",
    "print(f\"Average character count for a node: {average_node_char_count}\")\n",
    "print(f\"Average word count for a node: {average_node_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all metadata values with keys for one node\n",
    "for key in nodes[0].metadata.keys():\n",
    "    print(f\"{key}: {nodes[0].metadata[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Configuration for llm, embedding model, and node parsing\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "Settings.llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=300.0)\n",
    "Settings.embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embedding_model))\n",
    "\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    if documents:\n",
    "        print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "def build_index(client, documents, index_name):\n",
    "\n",
    "    chroma_collection = client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    print(f\"Created/existing collection {chroma_collection}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents,\n",
    "        transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# save documents to vector store\n",
    "def run_ingestion_pipeline(vectoredb_path, docs_path):\n",
    "    \n",
    "    print(\"Connecting to ChromaDB...\")\n",
    "    chromadb_client = chromadb.PersistentClient(path=vectoredb_path)\n",
    "\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents(docs_path)\n",
    "\n",
    "    print(\"Building index...\")\n",
    "    index = build_index(\n",
    "        client=chromadb_client, \n",
    "        documents=documents, \n",
    "        index_name=\"test\"\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data\"\n",
    "\n",
    "run_ingestion_pipeline(vectordb_path, docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# def retrieve_from_index(chunk_size, llm, embed_model, chromadb_client, index_name):\n",
    "def retrieve_from_index(chromadb_client, index_name):\n",
    "    chroma_collection = chromadb_client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        # chunk_size=chunk_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from llama_index.llms.ollama  import Ollama\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "# from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data\"\n",
    "index_name = \"test\"\n",
    "chunk_size = 500\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "print(\"Connecting to Chromadb\")\n",
    "chromadb_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "\n",
    "print(\"Loading Ollama...\")\n",
    "llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=300.0)\n",
    "\n",
    "print(\"Retrieving index...\")\n",
    "index = retrieve_from_index(chromadb_client, index_name)\n",
    "\n",
    "print(\"Constructing query engine...\")\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index=index,\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is Buddhism?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
