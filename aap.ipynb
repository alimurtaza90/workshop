{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 232 documents\n",
      "First document: Doc ID: 70e45dc8-0321-4586-aad6-f3ae277004d9\n",
      "Text:\n"
     ]
    }
   ],
   "source": [
    "docs_path = \"./data/docs\"\n",
    "documents = load_documents(docs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents is a <class 'list'>, of length 232, where each element is a <class 'llama_index.core.schema.Document'> object\n"
     ]
    }
   ],
   "source": [
    "print(f\"documents is a {type(documents)}, of length {len(documents)}, where each element is a {type(documents[0])} object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count for a document: 127.12068965517241\n",
      "Average word count per paragraph: 70.05225653206651\n",
      "Longest paragraph: 320\n",
      "Total number of paragraphs: 421\n",
      "Average number of paragraphs per document: 1.8146551724137931\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern for paragraphs and newlines\n",
    "split_pattern = r\"\\r?\\n\\s*\\r?\\n+\"\n",
    "\n",
    "# Initialize lists to store the word counts of all chunks (now paragraphs) and entire texts across all documents\n",
    "chunk_word_counts = []\n",
    "entire_text_word_counts = []\n",
    "\n",
    "# Initialize a variable to count the total number of paragraphs\n",
    "total_paragraph_count = 0\n",
    "\n",
    "# Iterate through each Document object in your list of documents\n",
    "for doc in documents:\n",
    "    # Assuming doc.text contains the full text of the PDF document\n",
    "    paragraphs = re.split(split_pattern, doc.text)\n",
    "    paragraphs = [paragraph.strip() for paragraph in paragraphs if paragraph.strip()]\n",
    "\n",
    "    # Update the total paragraph count\n",
    "    total_paragraph_count += len(paragraphs)\n",
    "\n",
    "    # Calculate the number of words in each paragraph and store it\n",
    "    chunk_word_counts.extend([len(paragraph.split()) for paragraph in paragraphs])\n",
    "\n",
    "    # Calculate the number of words in the entire text and store it\n",
    "    entire_word_count = len(doc.text.split())\n",
    "    entire_text_word_counts.append(entire_word_count)\n",
    "\n",
    "# Calculate summary statistics for paragraphs\n",
    "average_paragraph_word_count = sum(chunk_word_counts) / len(chunk_word_counts)\n",
    "max_paragraph_word_count = max(chunk_word_counts)\n",
    "\n",
    "# Calculate average word count for entire texts\n",
    "average_entire_text_word_count = sum(entire_text_word_counts) / len(entire_text_word_counts)\n",
    "\n",
    "# Calculate the average number of paragraphs per document\n",
    "average_paragraphs_per_document = total_paragraph_count / len(documents)\n",
    "\n",
    "print(f\"Average word count for a document: {average_entire_text_word_count}\")\n",
    "print(f\"Average word count per paragraph: {average_paragraph_word_count}\")\n",
    "print(f\"Longest paragraph: {max_paragraph_word_count}\")\n",
    "print(f\"Total number of paragraphs: {total_paragraph_count}\")\n",
    "print(f\"Average number of paragraphs per document: {average_paragraphs_per_document}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 244\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(\n",
    "    # paragraph_separator=r\"\\r?\\n\\s*\\r?\\n+\", \n",
    "    chunk_size=512, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "nodes = text_splitter.get_nodes_from_documents(documents)\n",
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character count for a node: 678.0286885245902\n",
      "Average word count for a node: 120.94672131147541\n"
     ]
    }
   ],
   "source": [
    "# checking the character count and word count for each node and getting avergaes\n",
    "node_char_counts = [len(node.text) for node in nodes]\n",
    "node_word_counts = [len(node.text.split()) for node in nodes]\n",
    "\n",
    "average_node_char_count = sum(node_char_counts) / len(node_char_counts)\n",
    "average_node_word_count = sum(node_word_counts) / len(node_word_counts)\n",
    "\n",
    "print(f\"Average character count for a node: {average_node_char_count}\")\n",
    "print(f\"Average word count for a node: {average_node_word_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: art of simple living.pdf\n",
      "file_path: c:\\Users\\ALIMURTA\\Documents\\Work\\Internal\\GenAI Tech Workshop Group\\Kickoff session\\Workshop\\workshop\\data\\docs\\art of simple living.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 953694\n",
      "creation_date: 2024-03-28\n",
      "last_modified_date: 2024-03-17\n"
     ]
    }
   ],
   "source": [
    "# print all metadata values with keys for one node\n",
    "for key in nodes[0].metadata.keys():\n",
    "    print(f\"{key}: {nodes[0].metadata[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Configuration for llm, embedding model, and node parsing\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "Settings.llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=300.0)\n",
    "Settings.embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=embedding_model))\n",
    "\n",
    "\n",
    "def load_documents(docs_path):\n",
    "    documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    if documents:\n",
    "        print(f\"First document: {documents[0]}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "\n",
    "\n",
    "def build_index(client, documents, index_name):\n",
    "\n",
    "    chroma_collection = client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    print(f\"Created/existing collection {chroma_collection}\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents,\n",
    "        transformations=[SentenceSplitter(chunk_size=512, chunk_overlap=20)],\n",
    "        storage_context=storage_context,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# save documents to vector store\n",
    "def run_ingestion_pipeline(vectoredb_path, docs_path):\n",
    "    \n",
    "    print(\"Connecting to ChromaDB...\")\n",
    "    chromadb_client = chromadb.PersistentClient(path=vectoredb_path)\n",
    "\n",
    "    print(\"Loading documents...\")\n",
    "    documents = load_documents(docs_path)\n",
    "\n",
    "    print(\"Building index...\")\n",
    "    index = build_index(\n",
    "        client=chromadb_client, \n",
    "        documents=documents, \n",
    "        index_name=\"test\"\n",
    "    )\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to ChromaDB...\n",
      "Loading documents...\n",
      "Loaded 232 documents\n",
      "First document: Doc ID: 9ec08f43-646f-46bc-8406-81267d8beb60\n",
      "Text:\n",
      "Building index...\n",
      "Created/existing collection name='test' id=UUID('e59cbf19-b47f-48e8-8f01-550bdf6a3579') metadata={'hnsw:space': 'cosine'} tenant='default_tenant' database='default_database'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d590ab0a5a45dd817674aaa720894f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/232 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f660c4fc8ebe4e14ac20af5c8b9cfc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/244 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x211cc37d690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data/docs\"\n",
    "\n",
    "run_ingestion_pipeline(vectordb_path, docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "# def retrieve_from_index(chunk_size, llm, embed_model, chromadb_client, index_name):\n",
    "def retrieve_from_index(chromadb_client, index_name):\n",
    "    chroma_collection = chromadb_client.get_or_create_collection(index_name, metadata={\"hnsw:space\": \"cosine\"})\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        # chunk_size=chunk_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Chromadb\n",
      "Loading Ollama...\n",
      "Retrieving index...\n",
      "Constructing query engine...\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from llama_index.llms.ollama  import Ollama\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "# from llama_index.core.settings import Settings\n",
    "\n",
    "\n",
    "vectordb_path = \"./data/vectordb\"\n",
    "docs_path = \"./data\"\n",
    "index_name = \"test\"\n",
    "chunk_size = 500\n",
    "ollama_model = \"mistral\"\n",
    "ollama_base_url = \"http://127.0.0.1:11434\"\n",
    "embedding_model = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "\n",
    "print(\"Connecting to Chromadb\")\n",
    "chromadb_client = chromadb.PersistentClient(path=vectordb_path)\n",
    "\n",
    "print(\"Loading Ollama...\")\n",
    "llm = Ollama(model=ollama_model, base_url=ollama_base_url, temperature=0, request_timeout=500.0)\n",
    "\n",
    "print(\"Retrieving index...\")\n",
    "index = retrieve_from_index(chromadb_client, index_name)\n",
    "\n",
    "print(\"Constructing query engine...\")\n",
    "query_engine = CitationQueryEngine.from_args(\n",
    "    index=index,\n",
    "    llm=llm,\n",
    "    similarity_top_k=3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Buddhism is a Japanese philosophy derived from the Sanskrit word \"dhyana,\" meaning meditation (Source 1, D. T. Suzuki). It emphasizes experiencing life in the present moment and removing dualistic distinctions between \"I\" and \"you,\" as well as our spiritual and everyday activities (Source 1). The practice of Zen Buddhism involves learning through physical labor, such as cleaning, which strengthens the mind and enables us to wake up and become more aware (Source 2). Belief in an unconditional protector, the Buddha, infuses us with tremendous energy and encourages us to keep going despite challenges (Source 3).\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is Buddhism?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:text_qa_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide an answer based solely on the provided sources. When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Every answer should include at least one source citation. Only cite a source when you are explicitly referencing it. If none of the sources are helpful, you should indicate that. For example:\n",
      "Source 1:\n",
      "The sky is red in the evening and blue in the morning.\n",
      "Source 2:\n",
      "Water is wet when the sky is red.\n",
      "Query: When is water wet?\n",
      "Answer: Water will be wet when the sky is red [2], which occurs in the evening [1].\n",
      "Now it's your turn. Below are several numbered sources of information:\n",
      "------\n",
      "{context_str}\n",
      "------\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: response_synthesizer:refine_template<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide an answer based solely on the provided sources. When referencing information from a source, cite the appropriate source(s) using their corresponding numbers. Every answer should include at least one source citation. Only cite a source when you are explicitly referencing it. If none of the sources are helpful, you should indicate that. For example:\n",
      "Source 1:\n",
      "The sky is red in the evening and blue in the morning.\n",
      "Source 2:\n",
      "Water is wet when the sky is red.\n",
      "Query: When is water wet?\n",
      "Answer: Water will be wet when the sky is red [2], which occurs in the evening [1].\n",
      "Now it's your turn. We have provided an existing answer: {existing_answer}Below are several numbered sources of information. Use them to refine the existing answer. If the provided sources are not helpful, you will repeat the existing answer.\n",
      "Begin refining!\n",
      "------\n",
      "{context_msg}\n",
      "------\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
